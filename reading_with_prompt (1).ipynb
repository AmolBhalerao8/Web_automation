{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573abba2-0fa6-4869-91f9-56a5eb45c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import pyautogui\n",
    "import undetected_chromedriver as uc\n",
    "#from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pyttsx3\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbee69a-1606-4501-9c09-ea462b1ba2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Command: search wild\n",
      "Google is not open. Opening now...\n",
      "Google is already open.\n",
      "Listening...\n",
      "Command: scroll down\n",
      "Listening...\n",
      "Could not understand the command.\n",
      "Could not understand the command.\n",
      "Listening...\n",
      "Could not understand the command.\n",
      "Could not understand the command.\n",
      "Listening...\n",
      "Command: scroll down\n",
      "Listening...\n",
      "Command: click official mine sota wild website\n",
      "No matching link found for: official mine sota wild website\n",
      "Listening...\n",
      "Command: click wild website\n",
      "Error clicking the link: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=133.0.6943.60); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x01250283+25139]\n",
      "\t(No symbol) [0x011DB234]\n",
      "\t(No symbol) [0x010B04A3]\n",
      "\t(No symbol) [0x010C1BE1]\n",
      "\t(No symbol) [0x010C0CD0]\n",
      "\t(No symbol) [0x010B7932]\n",
      "\t(No symbol) [0x010B5CDA]\n",
      "\t(No symbol) [0x010B8F9A]\n",
      "\t(No symbol) [0x010B9017]\n",
      "\t(No symbol) [0x010F303E]\n",
      "\t(No symbol) [0x0111CF3C]\n",
      "\t(No symbol) [0x010ED8E4]\n",
      "\t(No symbol) [0x0111D1B4]\n",
      "\t(No symbol) [0x0113E6E1]\n",
      "\t(No symbol) [0x0111CD36]\n",
      "\t(No symbol) [0x010EBD29]\n",
      "\t(No symbol) [0x010ED064]\n",
      "\tGetHandleVerifier [0x0155B143+3215603]\n",
      "\tGetHandleVerifier [0x015722BA+3310186]\n",
      "\tGetHandleVerifier [0x0156C4D2+3286146]\n",
      "\tGetHandleVerifier [0x012E9C80+654384]\n",
      "\t(No symbol) [0x011E45BD]\n",
      "\t(No symbol) [0x011E14A8]\n",
      "\t(No symbol) [0x011E1647]\n",
      "\t(No symbol) [0x011D3D20]\n",
      "\tBaseThreadInitThunk [0x76C85D49+25]\n",
      "\tRtlInitializeExceptionChain [0x7795CDEB+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x7795CD71+561]\n",
      "\n",
      "Listening...\n",
      "Command: enter reading mode\n",
      "Listening...\n",
      "Command: give me summary of this page\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 609\u001b[0m\n\u001b[0;32m    607\u001b[0m command \u001b[38;5;241m=\u001b[39m listen_command()\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m command:  \u001b[38;5;66;03m# Ensure command is not None\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m     handle_command(command)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not understand the command.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 367\u001b[0m, in \u001b[0;36mhandle_command\u001b[1;34m(command)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(action):  \u001b[38;5;66;03m# Ensures it's a function\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_argcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# If function takes no arguments\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m         action()\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m         action(command)\n",
      "Cell \u001b[1;32mIn[9], line 492\u001b[0m, in \u001b[0;36msummarize_this_page\u001b[1;34m()\u001b[0m\n\u001b[0;32m    489\u001b[0m article\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m    490\u001b[0m article\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m--> 492\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize_text(article\u001b[38;5;241m.\u001b[39mtext)  \u001b[38;5;66;03m# Generate summary using OpenAI\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary of this page:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Read summary aloud\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 183\u001b[0m, in \u001b[0;36msummarize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_text\u001b[39m(text):\n\u001b[1;32m--> 183\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    184\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Use GPT-4 or another suitable model\u001b[39;00m\n\u001b[0;32m    185\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    186\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that summarizes text.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    187\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize the following article:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    188\u001b[0m         ],\n\u001b[0;32m    189\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m# Adjust based on summary length preference\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     )\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    866\u001b[0m             {\n\u001b[0;32m    867\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    868\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    869\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    870\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    871\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    872\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    873\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    874\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    875\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    876\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    877\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    878\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    879\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    880\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    881\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    882\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    883\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    884\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    885\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    886\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    887\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    888\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    889\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    890\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    891\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    892\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    893\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    894\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    895\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    896\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    897\u001b[0m             },\n\u001b[0;32m    898\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    899\u001b[0m         ),\n\u001b[0;32m    900\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    901\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    902\u001b[0m         ),\n\u001b[0;32m    903\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    904\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    905\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    906\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    961\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    962\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    963\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    964\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    965\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    966\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1050\u001b[0m         input_options,\n\u001b[0;32m   1051\u001b[0m         cast_to,\n\u001b[0;32m   1052\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1053\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1054\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1055\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1099\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1100\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1101\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1102\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1103\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1104\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1050\u001b[0m         input_options,\n\u001b[0;32m   1051\u001b[0m         cast_to,\n\u001b[0;32m   1052\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1053\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1054\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1055\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1099\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1100\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1101\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1102\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1103\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1104\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1073\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# from openai import OpenAI\n",
    "# import openai\n",
    "# import speech_recognition as sr\n",
    "# import time\n",
    "# import pyautogui\n",
    "# import undetected_chromedriver as uc\n",
    "# #from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# import pyttsx3\n",
    "# import requests\n",
    "# from newspaper import Article\n",
    "# import re\n",
    "# import os\n",
    "# import subprocess\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "# import sys\n",
    "\n",
    "\n",
    "\n",
    "# Initialize speech recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Initialize global driver variable\n",
    "driver = None\n",
    "\n",
    "# OpenAI API Key\n",
    "# client = OpenAI(api_key = \"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\")\n",
    "client = OpenAI(api_key = \"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\")\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    \"\"\"Convert text to speech.\"\"\"\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def extract_paragraphs(url):\n",
    "    \"\"\"Extracts paragraphs from the page using BeautifulSoup.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "    return paragraphs\n",
    "\n",
    "def summarize_paragraph(paragraph):\n",
    "    \"\"\"Summarize a paragraph using newspaper3k.\"\"\"\n",
    "    article = Article('')\n",
    "    article.set_text(paragraph)\n",
    "    article.nlp()\n",
    "    return article.summary\n",
    "\n",
    "def listen_command():\n",
    "    \"\"\"Capture voice command\"\"\"\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=4)\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(f\"Command: {command}\")\n",
    "            return command\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand the command.\")\n",
    "            return None\n",
    "        except sr.RequestError:\n",
    "            print(\"Speech recognition service error.\")\n",
    "            return None\n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"Listening timeout.\")\n",
    "            return None\n",
    "\n",
    "def open_google():\n",
    "    \"\"\"Opens Google in a browser\"\"\"\n",
    "    global driver\n",
    "    if driver is None:\n",
    "        driver_path = ChromeDriverManager().install()\n",
    "        service = Service(driver_path)\n",
    "\n",
    "        # Launch undetected Chrome\n",
    "        driver = uc.Chrome(service = service)\n",
    "        driver.fullscreen_window()\n",
    "        driver.get(\"https://www.google.com\")\n",
    "    else:\n",
    "        print(\"Google is already open.\")\n",
    "\n",
    "\n",
    "def search_google(query):\n",
    "    global driver\n",
    "    if driver is not None:\n",
    "        open_google()\n",
    "        time.sleep(3)  # Wait for the browser to open\n",
    "\n",
    "        \n",
    "        try:\n",
    "            search_box = driver.find_element(By.NAME, \"q\")  # Find search box\n",
    "            search_box.clear()\n",
    "            search_box.send_keys(query)\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            time.sleep(2)  # Wait for results to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"Search box not found! Refreshing the page and retrying...\")\n",
    "            driver.refresh()\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        print(\"Google is not open. Opening now...\")\n",
    "        open_google()\n",
    "        time.sleep(2)\n",
    "        search_google(query)\n",
    "\n",
    "\n",
    "\n",
    "def scroll_down():\n",
    "    \"\"\"Scrolls down the webpage\"\"\"\n",
    "    global driver\n",
    "    if driver is not None:\n",
    "        driver.execute_script(\"window.scrollBy(0,500)\")\n",
    "    else:\n",
    "        print(\"Google is not open.\")\n",
    "\n",
    "\n",
    "def click_link_by_text(link_text):\n",
    "    \"\"\"Finds and clicks a link based on spoken words.\"\"\"\n",
    "    global driver\n",
    "    try:\n",
    "        elements = driver.find_elements(By.TAG_NAME, \"a\") + driver.find_elements(By.TAG_NAME, \"button\")\n",
    "        for element in elements:\n",
    "            if link_text in element.text.lower():\n",
    "                element.click()\n",
    "                print(f\"Clicked on: {element.text}\")\n",
    "                return\n",
    "        print(f\"No matching link found for: {link_text}\")\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No element found for {link_text}\")\n",
    "    except ElementClickInterceptedException:\n",
    "        print(f\"Element click intercepted for {link_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking the link: {e}\")\n",
    "\n",
    "def click(command):\n",
    "    link_text = command.split(\"click\", 1)[1].strip()\n",
    "    click_link_by_text(link_text)\n",
    "\n",
    "def scroll_up():\n",
    "    \"\"\"Scrolls up the page by 500 pixels.\"\"\"\n",
    "    driver.execute_script(\"window.scrollBy(0,-500)\")\n",
    "\n",
    "\n",
    "def close_google():\n",
    "    \"\"\"Closes the browser\"\"\"\n",
    "    global driver\n",
    "    if driver is not None:\n",
    "        driver.quit()\n",
    "        driver = None\n",
    "    else:\n",
    "        print(\"Google is not open.\")\n",
    "\n",
    "\n",
    "def extract_paragraphs(url):\n",
    "    article = Article(url)\n",
    "    article.download()  # Download the article\n",
    "    article.parse()  # Parse the article\n",
    "    return article.text.split('\\n')  # Split into paragraphs\n",
    "\n",
    "\n",
    "def save_summary_to_file(summary, url):\n",
    "    \"\"\"Saves the summary to a text file with a sanitized filename.\"\"\"\n",
    "    # Create a sanitized filename from the URL\n",
    "    filename = re.sub(r'https?://', '', url)  # Remove 'http://' or 'https://'\n",
    "    filename = re.sub(r'[\\/:*?\"<>|]', '_', filename)  # Replace invalid characters with '_'\n",
    "    filename = filename[:50]  # Limit filename length to avoid OS issues\n",
    "    # filepath = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", f\"{filename}.txt\")\n",
    "    filepath = os.path.join(os.getcwd(), f\"{filename}.txt\")  # Save in the current working directory\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Save the summary in the text file\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(summary)\n",
    "\n",
    "    print(f\"Summary saved as: {filepath}\")\n",
    "    \n",
    "def summarize_text(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Use GPT-4 or another suitable model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following article:\\n\\n{text}\"}\n",
    "        ],\n",
    "        max_tokens=200  # Adjust based on summary length preference\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "reading_mode = False  # Global flag for reading mode\n",
    "\n",
    "def process_command(command):\n",
    "    global reading_mode\n",
    "    reading_mode = True\n",
    "    speak(\"Reading mode activated. You can now use extra features like summarization.\")\n",
    "    \n",
    "\n",
    "def stop_reading_mode(command):\n",
    "    global reading_mode\n",
    "    reading_mode = False\n",
    "    speak(\"Reading mode deactivated.\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # elif \"summarize this page\" in command.lower():\n",
    "    #     if reading_mode:\n",
    "    #         summary = summarize_page()  # Assuming summarize_page() gets and processes the content\n",
    "    #         speak(\"Here is the summary.\")\n",
    "    #         print(summary)  # Display summary\n",
    "    #         save_summary_to_file(summary, url)  # Save summary if needed\n",
    "    #     else:\n",
    "    #         speak(\"Please enter into reading mode to get extra features.\")\n",
    "            \n",
    "    \n",
    "    # else:\n",
    "    #     speak(\"I didn't understand that command.\")\n",
    "\n",
    "\n",
    "def execute_gpt_action(gpt_response):\n",
    "    \"\"\"Executes the action suggested by GPT based on user input.\"\"\"\n",
    "    \n",
    "    gpt_response = gpt_response.lower().strip()\n",
    "\n",
    "    # Define possible actions based on GPT's response\n",
    "    actions = {\n",
    "        \"scroll down\": scroll_down,\n",
    "        \"scroll up\": scroll_up,\n",
    "        \"go back\": lambda: driver.back(),\n",
    "        \"search\": lambda cmd: search_google(cmd),\n",
    "        \"summarize page\": summarize_this_page,\n",
    "        \"take screenshot\": take_screenshot,\n",
    "        \"enter reading mode\": process_command,\n",
    "        \"stop reading mode\": stop_reading_mode,\n",
    "        \"summarize table\": summarize_table,\n",
    "        \"click\": click,\n",
    "        \"open link\": click_link_by_text,\n",
    "        \"exit program\": exit_program,\n",
    "    }\n",
    "\n",
    "    # Check if GPT response matches a known action\n",
    "    for action_text, function in actions.items():\n",
    "        if action_text in gpt_response:\n",
    "            if action_text == \"search\":\n",
    "            # If the action is search, we need to extract the search term\n",
    "                query = gpt_response.replace(\"search\", \"\").strip()  # Remove 'search' from the command\n",
    "                function(query)  # Pass the actual query to the function\n",
    "            elif function.__code__.co_argcount == 0:  # If function takes no arguments\n",
    "                function()\n",
    "            else:\n",
    "                function(action_text)\n",
    "            return\n",
    "            \n",
    "\n",
    "    # If GPT suggests a Google search\n",
    "    if \"search google for\" in gpt_response:\n",
    "        query = gpt_response.replace(\"search google for\", \"\").strip()\n",
    "        search_google(query)\n",
    "        return\n",
    "\n",
    "    # If GPT suggests opening a website\n",
    "    if \"open\" in gpt_response:\n",
    "        site_name = gpt_response.replace(\"open\", \"\").strip()\n",
    "        click_link_by_text(site_name)\n",
    "        return\n",
    "\n",
    "    # If GPT response is unclear\n",
    "    speak(\"Sorry, I couldn't understand the action. Can you rephrase?\")\n",
    "\n",
    "# def handle_command(command):\n",
    "#     \"\"\"Process the user command, handle known commands, and use GPT for unknown ones.\"\"\"\n",
    "    \n",
    "#     # Define direct command mappings\n",
    "#     open_sites = {\n",
    "#         \"google\": open_google,\n",
    "#     }\n",
    "\n",
    "#     command_variants = {\n",
    "#         (\"search\", \"look up\", \"find information on\", \"google this\"): lambda cmd: search_google(extract_query(cmd)),\n",
    "#         (\"scroll down\", \"go down\", \"move down\"): scroll_down,\n",
    "#         (\"scroll up\", \"go up\", \"move up\"): scroll_up,\n",
    "#         (\"go back\", \"back page\", \"previous page\", \"let's go to back page\"): lambda: driver.back(),\n",
    "#         (\"take screenshot\", \"capture screen\", \"save screenshot\"): take_screenshot,\n",
    "#         (\"summarize this page\", \"summarize\", \"give me summary of this\", \"explain this page\"): summarize_this_page,\n",
    "#         (\"enter reading mode\", \"activate reading mode\", \"open reading mode\", \"turn on reading mode\"): process_command,\n",
    "#         (\"summarize this table\", \"visualize this table\", \"what is this table\", \"give information about this table\"): summarize_table,\n",
    "#         (\"stop reading mode\", \"deactivate reading mode\", \"turn off reading mode\", \"exit reading mode\"): process_command,\n",
    "#         (\"exit\", \"exit program\", \"close google\"): close_google,\n",
    "#         (\"click\", \"open this link\"): click,\n",
    "\n",
    "#     }\n",
    "\n",
    "#     command_lower = command.lower().strip()\n",
    "\n",
    "#     # Handle \"open ...\" commands\n",
    "#     if command_lower.startswith(\"open\"):\n",
    "#         site_name = command_lower.replace(\"open\", \"\").strip()\n",
    "#         if site_name in open_sites:\n",
    "#             open_google()\n",
    "#             return\n",
    "#         else:\n",
    "#             click_link_by_text(site_name)\n",
    "#             return\n",
    "\n",
    "#     # Check for predefined command variations\n",
    "#     for keywords, action in command_variants.items():\n",
    "#         if any(keyword in command_lower for keyword in keywords):\n",
    "#             if action.__code__.co_argcount == 0:  # If function takes no arguments\n",
    "#                 action()\n",
    "#             else:\n",
    "#                 action(command)\n",
    "#             return\n",
    "\n",
    "\n",
    "#     # ðŸ”¹ Use GPT for Unrecognized Commands\n",
    "#     gpt_response = ask_gpt(f\"What action should be taken for: '{command}'?\")\n",
    "    \n",
    "#     if gpt_response:\n",
    "#         execute_gpt_action(gpt_response)\n",
    "#     else:\n",
    "#         speak(\"Sorry, I couldn't determine the action. Can you clarify?\")\n",
    "\n",
    "def handle_command(command):\n",
    "    \"\"\"Process the user command, handle known commands, and use GPT for unknown ones.\"\"\"\n",
    "    \n",
    "    # Define direct command mappings\n",
    "    open_sites = {\n",
    "        \"google\": open_google,\n",
    "    }\n",
    "\n",
    "    command_variants = {\n",
    "        (\"search\", \"look up\", \"find information on\", \"google this\"): lambda cmd: search_google(extract_query(cmd)),\n",
    "        (\"scroll down\", \"go down\", \"move down\"): scroll_down,\n",
    "        (\"scroll up\", \"go up\", \"move up\"): scroll_up,\n",
    "        (\"go back\", \"back page\", \"previous page\", \"let's go to back page\"): lambda: driver.back(),\n",
    "        (\"take screenshot\", \"capture screen\", \"save screenshot\"): take_screenshot,\n",
    "        (\"summarize this page\", \"summarize\", \"give me summary of this\", \"explain this page\"): summarize_this_page,\n",
    "        (\"enter reading mode\", \"activate reading mode\", \"open reading mode\", \"turn on reading mode\"): lambda cmd: process_command(cmd),\n",
    "        (\"summarize this table\", \"visualize this table\", \"what is this table\", \"give information about this table\"): summarize_table,\n",
    "        (\"stop reading mode\", \"deactivate reading mode\", \"turn off reading mode\", \"exit reading mode\"): lambda cmd: stop_reading_mode(cmd),\n",
    "        (\"exit program\", \"close google\"): close_google,\n",
    "        (\"click\", \"open this link\"): click,\n",
    "    }\n",
    "\n",
    "    command_lower = command.lower().strip()\n",
    "\n",
    "    # Handle \"open ...\" commands\n",
    "    if command_lower.startswith(\"open\"):\n",
    "        site_name = command_lower.replace(\"open\", \"\").strip()\n",
    "        if site_name in open_sites:\n",
    "            open_google()\n",
    "            return\n",
    "        else:\n",
    "            click_link_by_text(site_name)\n",
    "            return\n",
    "\n",
    "    # Check for predefined command variations\n",
    "    for keywords, action in command_variants.items():\n",
    "        if any(command_lower.startswith(keyword) or command_lower == keyword for keyword in keywords):\n",
    "            if callable(action):  # Ensures it's a function\n",
    "                if action.__code__.co_argcount == 0:  # If function takes no arguments\n",
    "                    action()\n",
    "                else:\n",
    "                    action(command)\n",
    "            return\n",
    "\n",
    "    # ðŸ”¹ Debugging Step: Print when GPT is used\n",
    "    print(f\"Unrecognized command: '{command}'. Sending to GPT...\")\n",
    "\n",
    "    # ðŸ”¹ Use GPT for Unrecognized Commands\n",
    "    #gpt_response = ask_gpt(f\"What action should be taken for: '{command}'?\")\n",
    "\n",
    "    gpt_response = ask_gpt(f\"Select the best action: search, enter reading mode, stop reading mode, scroll down, scroll up, go back, take screenshot, summarize page, summarize table, exit program, click. User said: '{command}'.\")\n",
    "\n",
    "\n",
    "    print(f\"GPT Response: {gpt_response}\")  # Debugging line\n",
    "\n",
    "    \n",
    "\n",
    "    if gpt_response:\n",
    "        execute_gpt_action(gpt_response)\n",
    "    else:\n",
    "        speak(\"Sorry, I couldn't determine the action. Can you clarify?\")\n",
    "\n",
    "\n",
    "def extract_query(command):\n",
    "    \"\"\"\n",
    "    Extracts the search query from the voice command.\n",
    "    Example: \"search for cats\" -> \"cats\"\n",
    "    \"\"\"\n",
    "    trigger_phrases = [\"search for\", \"google this\", \"look up\", \"search\"]\n",
    "    \n",
    "    for phrase in trigger_phrases:\n",
    "        if phrase in command:\n",
    "            return command.replace(phrase, \"\").strip()  # Remove the trigger phrase\n",
    "    \n",
    "    return command  # Fallback: use the entire command as the query\n",
    "        \n",
    "# def summarize_this_page():\n",
    "#     if reading_mode:\n",
    "#         url = driver.current_url\n",
    "#         article = Article(url)\n",
    "#         article.download()  # Download the article content\n",
    "#         article.parse()  # Parse the content\n",
    "\n",
    "#         summary = summarize_text(article.text)  # Use OpenAI instead of newspaper3k NLP\n",
    "#         print(f\"Summary of this page: {summary}\")\n",
    "#         time.sleep(1)\n",
    "#         save_summary_to_file(summary, url)\n",
    "        \n",
    "#         speak(\"summary saved to the system by this url name\")\n",
    "#     else:\n",
    "#         speak(\"Please enter into reading mode to get extra features.\")\n",
    "\n",
    "\n",
    "# def summarize_this_page():\n",
    "#     if reading_mode:\n",
    "#         url = driver.current_url\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "        \n",
    "#         summary = summarize_text(article.text)  # Generate summary using OpenAI\n",
    "#         print(f\"Summary of this page:\\n{summary}\")\n",
    "        \n",
    "#         # Read summary aloud\n",
    "#         speak(summary)\n",
    "        \n",
    "#         # Ask user if they want to save it\n",
    "#         speak(\"Do you want to save the summary to Notepad? Say yes or no.\")\n",
    "#         user_response = listen_command().lower()  # Convert response to lowercase\n",
    "\n",
    "#         if \"yes\" in user_response:\n",
    "#             save_summary_to_file(summary, url)\n",
    "#             speak(\"Summary saved successfully.\")\n",
    "#         else:\n",
    "#             speak(\"Okay, not saving the summary.\")\n",
    "\n",
    "#     else:\n",
    "#         speak(\"Please enter into reading mode to get extra features.\")\n",
    "\n",
    "\n",
    "\n",
    "# def summarize_this_page():\n",
    "#     if reading_mode:\n",
    "#         url = driver.current_url\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "        \n",
    "#         summary = summarize_text(article.text)  # Generate summary using OpenAI\n",
    "#         print(f\"Summary of this page:\\n{summary}\")\n",
    "\n",
    "#         # Read summary aloud\n",
    "#         speak(summary)\n",
    "        \n",
    "#         # Ask user if they want to save it\n",
    "#         speak(\"Do you want to save the summary to Notepad? Say yes or no.\")\n",
    "#         user_response = listen_command().lower()  # Convert response to lowercase\n",
    "\n",
    "#         if \"yes\" in user_response:\n",
    "#             filename = f\"summary_{url.replace('https://', '').replace('/', '_')}.txt\"\n",
    "#             with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "#                 file.write(summary)\n",
    "            \n",
    "#             speak(\"Summary saved successfully. Opening the file now.\")\n",
    "#             os.system(f'notepad.exe {filename}')  # Open file in Notepad\n",
    "#         else:\n",
    "#             speak(\"Okay, not saving the summary.\")\n",
    "\n",
    "#     else:\n",
    "#         speak(\"Please enter into reading mode to get extra features.\")\n",
    "\n",
    "\n",
    "notepad_process = None  # Global variable to track the Notepad process\n",
    "\n",
    "\n",
    "def summarize_this_page():\n",
    "    global notepad_process  # To manage the Notepad process\n",
    "\n",
    "    if reading_mode:\n",
    "        url = driver.current_url\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        summary = summarize_text(article.text)  # Generate summary using OpenAI\n",
    "        print(f\"Summary of this page:\\n{summary}\")\n",
    "\n",
    "        # Read summary aloud\n",
    "        speak(summary)\n",
    "\n",
    "        # Ask user if they want to save it\n",
    "        time.sleep(2)\n",
    "        speak(\"Do you want to save the summary to Notepad? Say yes or no.\")\n",
    "        user_response = listen_command() or \"\"  # Ensure it's never None\n",
    "        user_response = user_response.lower()\n",
    "\n",
    "        if \"yes\" in user_response:\n",
    "            try:\n",
    "                filename = f\"summary_{url.replace('https://', '').replace('/', '_')}.txt\"\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(summary)\n",
    "                \n",
    "                speak(\"Summary saved successfully. Opening Notepad now.\")\n",
    "                notepad_process = subprocess.Popen([\"notepad.exe\", filename])  # Open Notepad\n",
    "                \n",
    "                # Wait for a command to close Notepad\n",
    "                while True:\n",
    "                    #speak(\"Say 'close Notepad' to exit Notepad.\")\n",
    "                    user_response = listen_command() or \"\"  # Avoid None\n",
    "                    user_response = user_response.lower()\n",
    "\n",
    "                    if \"close notepad\" in user_response:\n",
    "                        notepad_process.terminate()\n",
    "                        close_notepad()\n",
    "                        break\n",
    "            \n",
    "            except Exception as e:\n",
    "                speak(\"An error occurred while saving the summary.\")\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        else:\n",
    "            speak(\"Okay, not saving the summary.\")\n",
    "\n",
    "    else:\n",
    "        speak(\"Please enter into reading mode to get extra features.\")\n",
    "\n",
    "def close_notepad():\n",
    "    global notepad_process\n",
    "    if notepad_process:\n",
    "        #notepad_process.terminate()  # Close Notepad\n",
    "        speak(\"Notepad closed.\")\n",
    "\n",
    "def close_notepad():\n",
    "    global notepad_process\n",
    "    if notepad_process:\n",
    "        notepad_process.terminate()  # Close Notepad\n",
    "        speak(\"Notepad closed.\")\n",
    "        notepad_process = None\n",
    "    else:\n",
    "        speak(\"Notepad is not open.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def take_screenshot():\n",
    "    print(\"Taking screenshot...\")\n",
    "    # subprocess.run([\"python\", \"llm.py\"])  # Runs llm.py when \"take screenshot\" is detected\n",
    "\n",
    "    result = subprocess.run([\"python\", \"llm.py\"], capture_output=True, text=True)\n",
    "    screenshot_path = result.stdout.strip()  # Get the printed path from llm.py\n",
    "    print(f\"Screenshot saved at: {screenshot_path}\")\n",
    "\n",
    "    # Run visualize.py with the screenshot path\n",
    "    # subprocess.run([\"python\", \"visualize.py\", screenshot_path])\n",
    "    time.sleep(5)\n",
    "            \n",
    "    vis_result = subprocess.run([\"python\", \"visualize.py\", screenshot_path], capture_output=True, text=True)\n",
    "    output = vis_result.stdout.strip()  # Get printed output from visualize.py\n",
    "    print(\"Output from visualize.py:\", output)\n",
    "\n",
    "def summarize_table():\n",
    "    if reading_mode:\n",
    "        result = subprocess.run([\"python\", \"llm.py\"], capture_output=True, text=True)\n",
    "        screenshot_path = result.stdout.strip()  # Get the printed path from llm.py\n",
    "        print(f\"Screenshot saved at: {screenshot_path}\")\n",
    "\n",
    "        # Run visualize.py with the screenshot path\n",
    "        # subprocess.run([\"python\", \"visualize.py\", screenshot_path])\n",
    "        time.sleep(5)\n",
    "            \n",
    "        # vis_result = subprocess.run([\"python\", \"visualize.py\"], capture_output=True, text=True)\n",
    "        # output = vis_result.stdout.strip()  # Get printed output from visualize.py\n",
    "        # print(\"Output from visualize.py:\", output)\n",
    "\n",
    "        # Start the subprocess with Popen\n",
    "        with open('visualize.log', 'r') as log_file:\n",
    "            log_output = log_file.read().strip()\n",
    "            print(\"Log output from visualize.py:\", log_output)\n",
    "                \n",
    "            \n",
    "    else:\n",
    "        speak(\"Please enter into reading mode to get extra features.\")\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.\",  # Use your desired model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def exit_program():\n",
    "    print(\"Exiting...\")\n",
    "    sys.exit()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Main loop to listen for commands\n",
    "while True:\n",
    "    command = listen_command()\n",
    "    if command:  # Ensure command is not None\n",
    "        handle_command(command)\n",
    "    else:\n",
    "        print(\"Could not understand the command.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14952e75-159f-488c-b5d8-ec6fd085876f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d711810-573f-4058-90bd-dd9f71c08b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe270f-6cc6-44c5-97f7-2bd467730fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
